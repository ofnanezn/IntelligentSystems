{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tic_tac_toe as game\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from keras.initializations import normal, identity\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD , Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_ACTIONS = 36\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVATION = 3200. # timesteps to observe before training\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 5000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(36, input_dim=36, activation='relu'))\n",
    "    model.add(Dense(36, activation='relu'))\n",
    "    model.add(Dense(36, activation='relu'))\n",
    "    model.add(Dense(36, activation='relu'))\n",
    "    model.add(Dense(36, activation='relu'))\n",
    "    \n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(model, args):\n",
    "    game_state = game.ofttt()\n",
    "    RM = deque()\n",
    "    (s_t, r_0, terminal) = (game_state.initial, 0, False)\n",
    "    player2 = game.random_player(game_state, game_state.initial)\n",
    "    if args['mode'] == 'run':\n",
    "        OBSERVE = 999999999    #Keep observe, never train\n",
    "        epsilon = FINAL_EPSILON    #Use a small epsilon to choose mainly policy actions\n",
    "        #Load model\n",
    "        print (\"Now we load weight\")  \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")\n",
    "    else:\n",
    "        #Assign an observation variable and max epsilon to train\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = INITIAL_EPSILON\n",
    "        \n",
    "    t = 0\n",
    "    \n",
    "    while(True):\n",
    "        #Initialize variables\n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0\n",
    "        a_t = np.zeros([N_ACTIONS])    #Output vector of actions a_[t] = 1 for action to take\n",
    "        \n",
    "        if random.random() <= epsilon:    #At the first move, choose randomly\n",
    "            print(\"----------Random Action----------\")\n",
    "            action_index = random.randrange(N_ACTIONS)\n",
    "            a_t[action_index] = 1\n",
    "        else:\n",
    "            q = model.predict(s_t)       #input the state at time t\n",
    "            max_Q = np.argmax(q)         #Take the max q value predicted from network\n",
    "            action_index = max_Q         #Assign action to the argmax Q\n",
    "            a_t[max_Q] = 1               #Output vector a_t = 1 for max_Q\n",
    "        \n",
    "        #Decrease epsilon by a smalll factor\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:               \n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000\n",
    "            \n",
    "        s_t1, r_t, terminal = game_state.next_state(action_index, s_t) #run the selected action and observed next state and reward\n",
    "        RM.append((s_t, action_index, r_t, s_t1, terminal))    # store the transition in the Replay Memory\n",
    "        if len(RM) > REPLAY_MEMORY:\n",
    "            RM.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE:\n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(RM, BATCH)\n",
    "            inputs = np.zeros((BATCH, 36))   #32, 80, 80, 4\n",
    "            print (inputs.shape)\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS)) \n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]\n",
    "                state_t1 = minibatch[i][3]\n",
    "                terminal = minibatch[i][4]\n",
    "                # if terminated, only equals reward\n",
    "\n",
    "                inputs[i:i + 1] = state_t    #I saved down s_t\n",
    "\n",
    "                targets[i] = model.predict(state_t)  # Hitting each buttom probability\n",
    "                Q_sa = model.predict(state_t1)\n",
    "\n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            # targets2 = normalize(targets)\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "\n",
    "        s_t = s_t1\n",
    "        t = t + 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
    "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
    "            \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game_state = game.ofttt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((-1, [[1, -1, -1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]), 1, True)\n"
     ]
    }
   ],
   "source": [
    "print(game_state.next_state(22,(1,[[1, -1, -1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(game_state.initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.randrange?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
